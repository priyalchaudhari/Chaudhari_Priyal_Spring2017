{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the article in Sports sections related to given 2 words \n",
    "## Using NLTK \n",
    "- Output of this is in csv file under Analysis 2 folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import glob\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "import operator\n",
    "from glob import glob\n",
    "import json\n",
    "import csv\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related words are: \n",
      "['champion', 'paladin', 'sub', 'hoagy', 'hoagie', 'hero', 'Italian sandwich', 'Cuban sandwich', 'torpedo', 'Heron', 'Hero', 'Hero of Alexandria', 'bomber', 'fighter', 'submarine', 'wedge', 'grinder', 'hero sandwich', 'poor boy', 'submarine sandwich', 'zep']\n",
      "['loosen', 'light', 'open', 'on the loose', 'lax', 'escaped', 'sluttish', 'idle', 'unloosen', 'unleash', 'let loose', 'liberal', 'wanton', 'loose', 'release', 'liberate', 'unaffixed', 'slack', 'unloose', 'relax', 'promiscuous', 'at large', 'informal', 'free', 'easy']\n"
     ]
    }
   ],
   "source": [
    "headline1 =list()\n",
    "headline2=list()\n",
    "word1 = \"hero\"               #given words\n",
    "word2 = \"loose\"\n",
    "Related_to_word1_list = getrelatedwords(word1)           #getting all the related words of those 2 words using synset wordnet package\n",
    "Related_to_word2_list = getrelatedwords(word2)\n",
    "findHeadline(Related_to_word1_list,Related_to_word2_list)\n",
    "saveToDisk(word1,word2)\n",
    "print(\"Related words are: \")\n",
    "print(Related_to_word1_list)\n",
    "print(Related_to_word2_list)\n",
    "#print(headline1)\n",
    "#print(headline2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path to Root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\priya\\midterm\\data\\Article search API\n"
     ]
    }
   ],
   "source": [
    "homedir = os.path.expanduser(\"~\")\n",
    "path_to_archive = homedir+\"\\\\midterm\\\\data\\\\Article search API\"\n",
    "print(path_to_archive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting All related words to given 2 words using wordnet corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getrelatedwords(word):\n",
    "    common_word_list = list()\n",
    "    for syn_sets in wn.synsets(word):\n",
    "        for syn in syn_sets.lemma_names():    # this will give you all synanym set of the word \n",
    "            common_word_list.append(syn.replace('_',' '))\n",
    "    common_word_list = set(common_word_list)\n",
    "    filtered_words= [word for word in common_word_list if word not in stopwords.words('english')] #filter the words and remove stopwords from it \n",
    "    return filtered_words          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the headlines related to that words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findHeadline(wl1,wl2):   # this function accepts the 2 word list\n",
    "    for section in glob(path_to_archive+'\\\\*'):\n",
    "        for subdirs, dirs, files in os.walk(section):\n",
    "            for file in files:\n",
    "                #try:\n",
    "                    with open(os.path.join(subdirs, file),'r', errors = 'ignore') as response:\n",
    "                        json_txt = json.load(response)\n",
    "                        for iter in range(len(json_txt[\"response\"][\"docs\"])):\n",
    "                            headline = json_txt[\"response\"][\"docs\"][iter][\"headline\"][\"main\"]  #getting headline for each file \n",
    "                            to_store = json_txt[\"response\"][\"docs\"][iter][\"headline\"][\"main\"]\n",
    "                            headline = headline.lower()\n",
    "                            headline = removePunctuation(headline)  #remove the punctuation from those headlines \n",
    "                            #print(headline)\n",
    "                            headline = [x for x in headline.split() if x not in stopwords.words('english')] # remove all the stopwords from headline \n",
    "                            for i in headline:\n",
    "                                if i in Related_to_word1_list:\n",
    "                                    l1=len(headline)\n",
    "                                    if (l1 > 0):\n",
    "                                        headline1.append(to_store)  # storing those headlines \n",
    "                                if i in Related_to_word2_list:\n",
    "                                    l2=len(headline)\n",
    "                                    if (l2>0):\n",
    "                                        headline2.append(to_store)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removePunctuation(unclear):\n",
    "    filtered = str.maketrans('','',string.punctuation)\n",
    "    return unclear.translate(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and writing csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saveToDisk(w1,w2):\n",
    "    if not os.path.exists(homedir+\"\\\\midterm\\\\Question 2\\\\Analysis_2\"):\n",
    "        os.makedirs(homedir+\"\\\\midterm\\\\Question 2\\\\Analysis_2\")\n",
    "    with open(homedir+'\\\\midterm\\\\Question 2\\\\Analysis_2\\\\Related.csv', 'w',newline='',encoding='utf-8') as myfile:                 # creating and opening a new excel sheet for storing data \n",
    "        wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "        wr.writerow([w1,w2]) #header\n",
    "        length = len(headline1) if len(headline1) > len(headline2) else len(headline2)\n",
    "        for i in range(length):\n",
    "            wr.writerow([headline1[i] if i< len(headline1)else \" \",headline2[i] if i < len(headline2) else \" \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
