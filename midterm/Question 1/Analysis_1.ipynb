{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import glob\n",
    "from email.parser import Parser\n",
    "import nltk\n",
    "import email\n",
    "import string\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic 1 contains a lot of meeting related words, perhaps they are from\n",
    "emails that were sent as meeting notices.\n",
    "2. Topic 2 while related to business seems to be more about the process\n",
    "rather than the content of the core business. It has a lot of terms\n",
    "relevant to business legalities.\n",
    "\n",
    "From the above plots, Topic 1 has a huge dip in Jan 1999, but bounces\n",
    "back up quickly and stays more or less constant until Jan 2001, after which\n",
    "it keeps increasing. The dip in Jan 1999 might be due to edge cases where\n",
    "emails related to Topic 1 got pushed over to Dec 1998 and Feb 1999, which\n",
    "6\n",
    "does seem to be the case by looking at the 2 spikes in the data at those\n",
    "times. The more interesting pattern is the steady increase in Topic 1 after\n",
    "Jan 2001, this is actually an interesting occurence. It is well known that\n",
    "Jerey Skilling and Kenneth Lay, then CEO and Chairman of Enron respec-\n",
    "tively at the time of the scandal, were holding regular meeting with their top\n",
    "executives in order to pressure them into \f",
    "nding new ways to hide Enron's\n",
    "debt. Since the scandal was only made known to the public in Oct 2001, it\n",
    "could well be the case that this abnormal rise in meetings was an indicator\n",
    "of Enron's executives trying to cover up their accounting fraud.\n",
    "Topic 2 on the other hand has a general decrease throughout the years.\n",
    "Since Topic 2 contains words like \"contract\", \"deal\" and \"agreement\", this\n",
    "might well be an indicator of dwindling business activity throughout the\n",
    "years. The huge decrease after Jan 2001 makes sense given that it was only\n",
    "8 months before the scandal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\priya\\midterm\\data\\enron\\maildir\\\n"
     ]
    }
   ],
   "source": [
    "homedir = os.path.expanduser(\"~\")\n",
    "path_to_mail = homedir+\"\\\\midterm\\\\data\\\\enron\\\\maildir\\\\\"\n",
    "print(path_to_mail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist_1999={'topic_1':1,'topic_2':1}\n",
    "dist_2000={'topic_1':1,'topic_2':1}\n",
    "dist_2001={'topic_1':1,'topic_2':1}\n",
    "filtered_data=[]\n",
    "demo=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'topic_2': 86, 'topic_1': 235}\n",
      "{'topic_2': 2024, 'topic_1': 5222}\n",
      "{'topic_2': 1857, 'topic_1': 6417}\n"
     ]
    }
   ],
   "source": [
    "convicted_emp_names = ['skilling-j','lay-k']\n",
    "topic1 = ['message','origin','pleas','please','email','thank','attach','file','copi','copy','inform','receiv','receive']\n",
    "topic2 = ['enron','deal','agreement','change','contract','corp','fax','houston','date','america']\n",
    "for name in convicted_emp_names:\n",
    "    user_mail= path_to_mail+name\n",
    "    #print(user_mail)\n",
    "    for subdir, dirs, files in os.walk(user_mail):\n",
    "        for file in files:       \n",
    "            if(file[0]!=\".\"):\n",
    "                try:\n",
    "                    response = open(os.path.join(subdir, file),'r', errors = 'ignore')\n",
    "                    fp=Parser().parsestr(response.read())\n",
    "                except:\n",
    "                    print('error',os.path.join(subdir, file))\n",
    "                    \n",
    "                unfiltered_word_list = nltk.word_tokenize(fp.get_payload()) \n",
    "                filtered_data=(removePunctuation(unfiltered_word_list))\n",
    "                ##print(filtered_data)\n",
    "                raw_date = fp['Date']\n",
    "                date = email.utils.parsedate(raw_date)\n",
    "                #print(date[0])\n",
    "                for i in filtered_data:\n",
    "                    if date[0] == 1999:\n",
    "                        #print('here')\n",
    "                        if i in topic1:\n",
    "                            dist_1999['topic_1'] = dist_1999['topic_1']+1\n",
    "                        if i in topic2:\n",
    "                            dist_1999['topic_2'] = dist_1999['topic_2']+1\n",
    "                    if date[0] == 2000:\n",
    "                        if i in topic1:\n",
    "                            dist_2000['topic_1'] = dist_2000['topic_1']+1\n",
    "                        if i in topic2:\n",
    "                            dist_2000['topic_2'] = dist_2000['topic_2']+1\n",
    "                    if date[0] == 2001:\n",
    "                        if i in topic1:\n",
    "                            dist_2001['topic_1'] = dist_2001['topic_1']+1\n",
    "                        if i in topic2:\n",
    "                            dist_2001['topic_2'] = dist_2001['topic_2']+1\n",
    "#sorted_characters = sorted(dist_1999.items(), key=operator.itemgetter(1), reverse = True) # sorting the dict based on frequency\n",
    "#print('sorted frequency list: \\n ',sorted_characters[:10])\n",
    "print(dist_1999)\n",
    "print(dist_2000)\n",
    "print(dist_2001)\n",
    "#print(demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removePunctuation(unclear):\n",
    "    filtered_words = [''.join(c for c in s if c not in string.punctuation) for s in unclear]\n",
    "    filtered_words = [s for s in filtered_words if s]\n",
    "    return filtered_words  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
